# DocDB v0.3 Horizontal Scaling Analysis: 3 vs 7 Databases

**Test Date**: January 29, 2026  
**Version**: v0.3  
**Duration**: 300 seconds (5 minutes)

## Executive Summary

**Key Finding**: Scaling horizontally from 3 to 7 databases (with 10 workers each) resulted in **degraded performance**, indicating **scheduler/pool contention**:

- **Throughput**: **627.8 ops/sec** (vs 672.3 ops/sec with 3 databases) = **-6.6% decrease**
- **P95 Latency**: **~222ms average** (vs ~110ms with 3 databases) = **+101.8% increase**
- **Total Operations**: **188,349** (vs 201,777) = **-6.6% decrease**
- **Per-Database Throughput**: **89.8 ops/sec** (vs 224.1 ops/sec) = **-59.9% decrease**

**Conclusion**: The scheduler/pool appears to be **contended when managing more databases**. Optimal configuration is **~3 databases with 10 workers each** for this workload.

---

## Test Configurations

| Configuration | Databases | Workers/DB | Total Workers | CRUD Mix            | Document Size |
| ------------- | --------- | ---------- | ------------- | ------------------- | ------------- |
| **Test 1**    | 3         | 10         | 30            | 40%R/30%C/20%U/10%D | 1024 bytes    |
| **Test 2**    | 7         | 10         | 70            | 40%R/30%C/20%U/10%D | 1024 bytes    |

---

## Performance Comparison: 3 vs 7 Databases

### Throughput Comparison

| Metric                 | 3 Databases | 7 Databases | Change        |
| ---------------------- | ----------- | ----------- | ------------- |
| **Total Ops (5m)**     | 201,777     | 188,349     | **-6.6%** ⬇️  |
| **Ops/sec**            | 672.3       | 627.8       | **-6.6%** ⬇️  |
| **Per-DB Ops/sec**     | 224.1       | 89.8        | **-59.9%** ⬇️ |
| **Ops per Worker/sec** | 22.4        | 9.0         | **-59.8%** ⬇️ |

**Analysis**:

- **Negative horizontal scaling**: More databases = less total throughput
- **Per-database throughput dropped 60%**: Each database getting much less service
- **Scheduler contention**: Queue management overhead increases with more databases
- **Per-worker efficiency dropped 60%**: Workers spending more time waiting/contending

### P95 Latency Comparison

| Operation   | 3 DBs (ms) | 7 DBs (ms) | Change         |
| ----------- | ---------- | ---------- | -------------- |
| **Create**  | 110.8      | 234.0      | **+111.2%** ⬆️ |
| **Read**    | 103.6      | 206.8      | **+99.6%** ⬆️  |
| **Update**  | 119.6      | 236.2      | **+97.5%** ⬆️  |
| **Delete**  | 106.8      | 214.0      | **+100.4%** ⬆️ |
| **Average** | **110.2**  | **222.8**  | **+102.2%** ⬆️ |

**Analysis**:

- **Latency doubled**: All operations ~100% slower
- **Consistent degradation**: All operation types affected similarly
- **Scheduler overhead**: More databases = more queue switching overhead

### P99 Latency Comparison

| Operation   | 3 DBs (ms) | 7 DBs (ms) | Change         |
| ----------- | ---------- | ---------- | -------------- |
| **Create**  | 148.5      | 313.3      | **+111.0%** ⬆️ |
| **Read**    | 138.4      | 279.3      | **+101.8%** ⬆️ |
| **Update**  | 157.2      | 314.2      | **+99.9%** ⬆️  |
| **Delete**  | 141.5      | 292.9      | **+107.0%** ⬆️ |
| **Average** | **146.4**  | **299.9**  | **+104.8%** ⬆️ |

**Analysis**:

- **Tail latency doubled**: P99 increased ~105%
- **Higher variance**: More extreme tail cases with more databases

### P50 (Median) Latency Comparison

| Operation   | 3 DBs (ms) | 7 DBs (ms) | Change         |
| ----------- | ---------- | ---------- | -------------- |
| **Create**  | 47.3       | 103.9      | **+119.7%** ⬆️ |
| **Read**    | 41.6       | 85.3       | **+105.0%** ⬆️ |
| **Update**  | 55.9       | 105.9      | **+89.4%** ⬆️  |
| **Delete**  | 45.1       | 90.0       | **+99.6%** ⬆️  |
| **Average** | **47.2**   | **96.1**   | **+103.6%** ⬆️ |

**Analysis**:

- **Median latency doubled**: Even typical operations much slower
- **Read operations worst affected**: +105% suggests read queue contention

---

## Per-Database Analysis (7 Databases)

### Operations Distribution

| Database  | Total Ops   | Ops/sec   | Create      | Read         | Update      | Delete      |
| --------- | ----------- | --------- | ----------- | ------------ | ----------- | ----------- |
| **db1**   | 32,786      | 109.3     | 9,827 (30%) | 13,077 (40%) | 6,498 (20%) | 3,384 (10%) |
| **db2**   | 32,494      | 108.3     | 9,761 (30%) | 13,143 (40%) | 6,444 (20%) | 3,146 (10%) |
| **db3**   | 32,518      | 108.4     | 9,695 (30%) | 13,057 (40%) | 6,590 (20%) | 3,176 (10%) |
| **d5**    | 30,431      | 101.4     | 9,025 (30%) | 12,271 (40%) | 6,101 (20%) | 3,034 (10%) |
| **d6**    | 30,012      | 100.0     | 8,999 (30%) | 12,006 (40%) | 6,046 (20%) | 2,961 (10%) |
| **d7**    | 30,108      | 100.4     | 9,021 (30%) | 12,006 (40%) | 6,168 (20%) | 2,913 (10%) |
| **Total** | **188,349** | **627.8** | **56,328**  | **75,560**   | **37,847**  | **18,614**  |

**Analysis**:

- **Good load balancing**: Operations distributed across databases (<9% variance)
- **CRUD distribution correct**: Matches expected 40/30/20/10 mix
- **New databases (d5-d7) slightly lower**: May be due to startup overhead or scheduler fairness

### Per-Database Latency (P95) - 7 Databases

| Database    | Create (ms) | Read (ms) | Update (ms) | Delete (ms) |
| ----------- | ----------- | --------- | ----------- | ----------- |
| **db1**     | 217.3       | 199.4     | 238.6       | 205.1       |
| **db2**     | 217.1       | 198.0     | 237.1       | 210.2       |
| **db3**     | 219.6       | 200.7     | 234.3       | 207.8       |
| **d5**      | 246.6       | 214.4     | 234.0       | 219.3       |
| **d6**      | 257.5       | 219.3     | 240.8       | 226.5       |
| **d7**      | 249.7       | 211.0     | 232.2       | 217.1       |
| **Average** | **234.0**   | **206.8** | **236.2**   | **214.0**   |

**Analysis**:

- **Consistent across databases**: <10% variance
- **New databases (d5-d7) slightly higher latency**: May indicate scheduler queue depth issues
- **Update operations slowest**: Consistent with write contention

---

## Comprehensive Scaling Analysis

### Vertical vs Horizontal Scaling Comparison

| Metric               | 3 DBs × 10W | 3 DBs × 20W | 7 DBs × 10W | Best        |
| -------------------- | ----------- | ----------- | ----------- | ----------- |
| **Total Ops/sec**    | 672.3       | 619.7       | 627.8       | **3×10** ✅ |
| **P95 Latency (ms)** | 110.2       | 182.0       | 222.8       | **3×10** ✅ |
| **Per-DB Ops/sec**   | 224.1       | 206.6       | 89.8        | **3×10** ✅ |
| **Ops/Worker/sec**   | 22.4        | 10.3        | 9.0         | **3×10** ✅ |

**Key Findings**:

1. **3 databases × 10 workers is optimal** for this workload
2. **Vertical scaling (more workers)**: -7.8% throughput, +65% latency
3. **Horizontal scaling (more databases)**: -6.6% throughput, +102% latency
4. **Horizontal scaling worse for latency**: Doubled latency vs 65% increase with vertical scaling

### Scaling Efficiency

#### Vertical Scaling (3 DBs, 10→20 workers)

```
Workers:  30 → 60  (2x increase)
Throughput: 672 → 620 ops/sec  (-7.8% decrease)
Scaling Efficiency: -0.039x
```

#### Horizontal Scaling (10W/DB, 3→7 databases)

```
Databases:  3 → 7  (2.33x increase)
Workers:  30 → 70  (2.33x increase)
Throughput: 672 → 628 ops/sec  (-6.6% decrease)
Scaling Efficiency: -0.028x
```

**Conclusion**: Both scaling approaches show **negative efficiency**, but horizontal scaling has **worse latency impact**.

---

## Root Cause Analysis

### Likely Bottlenecks (Horizontal Scaling)

1. **Scheduler Contention**
   - Queue-depth fairness algorithm may have overhead with more databases
   - More databases = more queue scanning/switching overhead
   - Ants pool may have internal contention with more concurrent tasks

2. **Pool-Level Lock Contention**
   - Single scheduler pool managing more databases
   - Queue management locks may be contended
   - Worker selection overhead increases

3. **Memory/GC Pressure**
   - More databases = more metadata/state to manage
   - More concurrent transactions = more memory pressure
   - GC pauses may increase

4. **I/O Contention**
   - More databases writing to WAL simultaneously
   - WAL group commit may have contention with more databases
   - Data file writes may be serialized

### Evidence

- **Per-database throughput dropped 60%**: Each database getting much less service
- **Per-worker efficiency dropped 60%**: Workers spending more time waiting
- **Latency doubled**: Scheduler overhead is significant
- **New databases (d5-d7) have higher latency**: Scheduler queue depth issues

---

## Recommendations

### 1. Optimal Configuration

- **Current optimal**: **3 databases × 10 workers** = 30 total workers
- **Avoid**: Scaling beyond 3-4 databases with current scheduler design
- **Consider**: Per-database scheduler pools if horizontal scaling needed

### 2. Scheduler Optimizations

- **Consider**: Per-database worker pools instead of single shared pool
- **Consider**: Optimizing queue selection algorithm for many databases
- **Consider**: Batching queue operations to reduce overhead

### 3. Pool Architecture

- **Current**: Single shared scheduler pool for all databases
- **Alternative**: Separate pools per database (better isolation)
- **Trade-off**: More overhead vs better scalability

### 4. Testing Recommendations

- **Test**: 2, 4, 5, 6 databases to find breaking point
- **Test**: Per-database pools vs shared pool
- **Monitor**: Scheduler metrics (queue depths, worker utilization)

### 5. Workload-Specific Guidance

- **Small deployments (1-3 DBs)**: Current architecture optimal
- **Medium deployments (4-6 DBs)**: May need scheduler optimizations
- **Large deployments (7+ DBs)**: Consider per-database pools

---

## Performance Summary

### 3 Databases × 10 Workers (Optimal) ✅

- **672 ops/sec** - Excellent throughput
- **~110ms P95** - Excellent latency
- **224 ops/DB/sec** - High per-database throughput
- **22.4 ops/worker/sec** - High efficiency

### 7 Databases × 10 Workers (Degraded) ⚠️

- **628 ops/sec** - Lower throughput (-6.6%)
- **~223ms P95** - Much higher latency (+102%)
- **90 ops/DB/sec** - Low per-database throughput (-60%)
- **9.0 ops/worker/sec** - Low efficiency (-60%)

### 3 Databases × 20 Workers (Degraded) ⚠️

- **620 ops/sec** - Lower throughput (-7.8%)
- **~182ms P95** - Higher latency (+65%)
- **207 ops/DB/sec** - Moderate per-database throughput
- **10.3 ops/worker/sec** - Low efficiency (-54%)

---

## Conclusion

**v0.3 demonstrates excellent performance at 3 databases × 10 workers**, but shows **clear degradation when scaling horizontally to 7 databases**. The scheduler/pool appears to be **contended when managing more databases**.

**Key Takeaways**:

1. **Optimal configuration**: 3 databases × 10 workers = 30 total workers
2. **Horizontal scaling limit**: System does not scale well beyond ~3-4 databases
3. **Scheduler bottleneck**: Queue management overhead increases significantly
4. **Latency impact**: Horizontal scaling worse for latency than vertical scaling
5. **Consistency**: Load balancing remains good even under contention

**Next Steps**:

- Profile scheduler overhead with more databases
- Consider per-database scheduler pools for horizontal scaling
- Test intermediate database counts (2, 4, 5, 6)
- Monitor scheduler metrics (queue depths, worker wait times)
- Optimize queue selection algorithm for many databases

---

## Test Artifacts

- **Results JSON**: `multidb_results-2.json` (7 databases test)
- **Previous Results**:
  - `V0.3_RESULTS.md` (3 databases × 10 workers)
  - `V0.3_SCALABILITY_ANALYSIS.md` (3 databases × 20 workers)
- **Test Command (7 databases)**:
  ```bash
  go run docdb/tests/load/cmd/multidb_loadtest/main.go \
    -databases db1,db2,db3,d5,d6,d7 \
    -workers-per-db 10 \
    -duration 5m \
    -socket /tmp/docdb.sock \
    -wal-dir ./docdb/data/wal \
    -output multidb_results-2.json \
    -csv
  ```
