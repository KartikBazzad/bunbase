# DocDB v0.3 Scalability Analysis: 10 vs 20 Workers per Database

**Test Date**: January 29, 2026  
**Version**: v0.3  
**Duration**: 300 seconds (5 minutes)

## Executive Summary

**Key Finding**: Doubling workers from 10 to 20 per database resulted in **degraded performance**, indicating **saturation/contention** rather than linear scaling:

- **Throughput**: **619.7 ops/sec** (vs 672.3 ops/sec with 10 workers) = **-7.8% decrease**
- **P95 Latency**: **~182ms average** (vs ~110ms with 10 workers) = **+65.5% increase**
- **Total Operations**: **185,903** (vs 201,777) = **-7.9% decrease**

**Conclusion**: The system appears to be **I/O or lock-contention bound** rather than CPU-bound. Optimal worker count for this workload is **~10 workers per database**.

---

## Test Configurations

| Configuration | Workers/DB | Total Workers | CRUD Mix            | Document Size |
| ------------- | ---------- | ------------- | ------------------- | ------------- |
| **Test 1**    | 10         | 30            | 40%R/30%C/20%U/10%D | 1024 bytes    |
| **Test 2**    | 20         | 60            | 40%R/30%C/20%U/10%D | 1024 bytes    |

---

## Performance Comparison: 10 vs 20 Workers

### Throughput Comparison

| Metric                 | 10 Workers | 20 Workers | Change        |
| ---------------------- | ---------- | ---------- | ------------- |
| **Total Ops (5m)**     | 201,777    | 185,903    | **-7.9%** ⬇️  |
| **Ops/sec**            | 672.3      | 619.7      | **-7.8%** ⬇️  |
| **Per-DB Ops/sec**     | 224.1      | 206.6      | **-7.8%** ⬇️  |
| **Ops per Worker/sec** | 22.4       | 10.3       | **-54.0%** ⬇️ |

**Analysis**:

- **Negative scaling**: More workers = less throughput
- **Per-worker efficiency dropped 54%**: Each worker is doing less work due to contention
- **Saturation point**: System is bottlenecked, likely on I/O or lock contention

### P95 Latency Comparison

| Operation   | 10 Workers (ms) | 20 Workers (ms) | Change        |
| ----------- | --------------- | --------------- | ------------- |
| **Create**  | 110.8           | 181.7           | **+64.0%** ⬆️ |
| **Read**    | 103.6           | 174.4           | **+68.3%** ⬆️ |
| **Update**  | 119.6           | 192.9           | **+61.3%** ⬆️ |
| **Delete**  | 106.8           | 180.0           | **+68.5%** ⬆️ |
| **Average** | **110.2**       | **182.0**       | **+65.2%** ⬆️ |

**Analysis**:

- **Significant latency increase**: All operations ~65% slower
- **Consistent degradation**: All operation types affected similarly
- **Contention overhead**: More workers competing for shared resources

### P99 Latency Comparison

| Operation   | 10 Workers (ms) | 20 Workers (ms) | Change        |
| ----------- | --------------- | --------------- | ------------- |
| **Create**  | 148.5           | 261.1           | **+75.8%** ⬆️ |
| **Read**    | 138.4           | 249.1           | **+80.0%** ⬆️ |
| **Update**  | 157.2           | 270.7           | **+72.2%** ⬆️ |
| **Delete**  | 141.5           | 258.9           | **+83.0%** ⬆️ |
| **Average** | **146.4**       | **260.0**       | **+77.6%** ⬆️ |

**Analysis**:

- **Tail latency worse**: P99 increased more than P95 (~78% vs ~65%)
- **Higher variance**: More contention leads to more extreme tail cases

### P50 (Median) Latency Comparison

| Operation   | 10 Workers (ms) | 20 Workers (ms) | Change         |
| ----------- | --------------- | --------------- | -------------- |
| **Create**  | 47.3            | 96.9            | **+104.9%** ⬆️ |
| **Read**    | 41.6            | 91.9            | **+120.9%** ⬆️ |
| **Update**  | 55.9            | 105.7           | **+89.1%** ⬆️  |
| **Delete**  | 45.1            | 95.4            | **+111.5%** ⬆️ |
| **Average** | **47.2**        | **97.2**        | **+106.0%** ⬆️ |

**Analysis**:

- **Median latency doubled**: Even typical operations are much slower
- **Read operations worst affected**: +121% increase suggests read contention

---

## Per-Database Analysis (20 Workers)

### Operations Distribution

| Database  | Total Ops   | Ops/sec   | Create       | Read         | Update       | Delete      |
| --------- | ----------- | --------- | ------------ | ------------ | ------------ | ----------- |
| **db1**   | 62,205      | 207.4     | 18,681 (30%) | 24,886 (40%) | 12,578 (20%) | 6,060 (10%) |
| **db2**   | 61,109      | 203.7     | 18,381 (30%) | 24,287 (40%) | 12,403 (20%) | 6,038 (10%) |
| **db3**   | 62,589      | 208.6     | 18,459 (30%) | 25,260 (40%) | 12,530 (20%) | 6,340 (10%) |
| **Total** | **185,903** | **619.7** | **55,521**   | **74,433**   | **37,511**   | **18,438**  |

**Analysis**:

- **Good load balancing**: Operations evenly distributed (<2% variance)
- **CRUD distribution correct**: Matches expected 40/30/20/10 mix

### Per-Database Latency (P95) - 20 Workers

| Database    | Create (ms) | Read (ms) | Update (ms) | Delete (ms) |
| ----------- | ----------- | --------- | ----------- | ----------- |
| **db1**     | 180.5       | 173.8     | 192.4       | 179.3       |
| **db2**     | 183.7       | 176.4     | 197.2       | 180.7       |
| **db3**     | 180.8       | 173.1     | 189.1       | 180.0       |
| **Average** | **181.7**   | **174.4** | **192.9**   | **180.0**   |

**Analysis**:

- **Consistent across databases**: <3ms variance
- **Update operations slowest**: Consistent with write contention

---

## WAL Growth Analysis

### 10 Workers vs 20 Workers

| Database  | 10 Workers Growth (MB/s) | 20 Workers Growth (MB/s) | Change    |
| --------- | ------------------------ | ------------------------ | --------- |
| **db1**   | 0.055                    | 0.054                    | -1.8%     |
| **db2**   | 0.055                    | 0.054                    | -1.8%     |
| **db3**   | 0.055                    | 0.054                    | -1.8%     |
| **Total** | **0.166**                | **0.162**                | **-2.4%** |

**Analysis**:

- **WAL growth similar**: Slightly lower with 20 workers (fewer total operations)
- **Not I/O bound**: WAL write rate is not the bottleneck

---

## Scalability Analysis

### Throughput Scaling

```
Workers:  10 → 20  (2x increase)
Throughput: 672 → 620 ops/sec  (-7.8% decrease)

Scaling Efficiency: -7.8% / 100% = -0.078x
```

**Conclusion**: **Negative scaling** - system performance degrades with more workers.

### Latency Scaling

```
Workers:  10 → 20  (2x increase)
P95 Latency: 110ms → 182ms  (+65.2% increase)

Latency Degradation: +65.2% / 100% = +0.65x
```

**Conclusion**: **Significant latency degradation** - contention increases wait times.

---

## Root Cause Analysis

### Likely Bottlenecks

1. **Lock Contention**
   - `LogicalDB` write locks may be contended with more concurrent writers
   - Index shard locks (512 shards) may still see contention hotspots
   - WAL writer lock contention

2. **I/O Saturation**
   - Data file writes may be serialized
   - WAL fsync operations (even with group commit) may be bottlenecked
   - Disk I/O queue depth limits

3. **Scheduler Overhead**
   - More workers = more context switching
   - Queue management overhead increases
   - Ants pool may have internal contention

4. **Memory Pressure**
   - More concurrent transactions = more memory usage
   - GC pressure may increase with more goroutines

### Evidence

- **Per-worker efficiency drops 54%**: Workers are waiting/contending rather than working
- **Median latency doubled**: Even typical operations affected, suggesting lock contention
- **Read operations worst affected**: Read locks may be contended despite MVCC
- **WAL growth similar**: I/O throughput not saturated, suggesting lock contention

---

## Recommendations

### 1. Optimal Worker Count

- **Current optimal**: **~10 workers per database** for this workload
- **Monitor**: Test with 5, 8, 12, 15 workers to find sweet spot

### 2. Reduce Lock Contention

- **Consider**: Further narrowing `LogicalDB` write lock scope
- **Consider**: Increasing index shard count beyond 512 if hotspots detected
- **Consider**: Read-write locks instead of exclusive locks where possible

### 3. I/O Optimization

- **Monitor**: Disk I/O queue depth and latency
- **Consider**: Async/background WAL fsync if not already optimized
- **Consider**: Data file write batching if possible

### 4. Scheduler Tuning

- **Monitor**: Ants pool metrics (running/waiting/free workers)
- **Consider**: Adjusting `WorkerExpiry` and `PreAlloc` settings
- **Consider**: Per-database worker pools if cross-DB contention exists

### 5. Workload-Specific Tuning

- **Read-heavy workloads**: May benefit from more workers (if read locks allow)
- **Write-heavy workloads**: May need fewer workers to reduce contention
- **Mixed workloads**: Current 10 workers appears optimal

---

## Performance Summary

### 10 Workers (Optimal)

- ✅ **672 ops/sec** - Excellent throughput
- ✅ **~110ms P95** - Excellent latency
- ✅ **22.4 ops/worker/sec** - High efficiency

### 20 Workers (Degraded)

- ⚠️ **620 ops/sec** - Lower throughput (-7.8%)
- ⚠️ **~182ms P95** - Higher latency (+65%)
- ⚠️ **10.3 ops/worker/sec** - Low efficiency (-54%)

---

## Conclusion

**v0.3 demonstrates excellent performance at 10 workers per database**, but shows **clear saturation/contention when scaled to 20 workers**. The system is likely **lock-contention bound** rather than CPU or I/O bound.

**Key Takeaways**:

1. **Optimal worker count**: ~10 workers per database for current workload
2. **Scaling limit**: System does not scale linearly beyond ~10 workers
3. **Contention**: Lock contention appears to be the primary bottleneck
4. **Consistency**: Load balancing and consistency remain excellent even under contention

**Next Steps**:

- Profile lock contention to identify hotspots
- Test intermediate worker counts (5, 8, 12, 15) to find optimal point
- Consider lock-free or fine-grained locking optimizations
- Monitor ants pool and scheduler metrics under different loads

---

## Test Artifacts

- **Results JSON**: `multidb_results-1.json` (20 workers test)
- **Previous Results**: `V0.3_RESULTS.md` (10 workers test)
- **Test Command (20 workers)**:
  ```bash
  go run docdb/tests/load/cmd/multidb_loadtest/main.go \
    -databases db1,db2,db3 \
    -workers-per-db 20 \
    -duration 5m \
    -socket /tmp/docdb.sock \
    -wal-dir ./docdb/data/wal \
    -output multidb_results-1.json \
    -csv
  ```
